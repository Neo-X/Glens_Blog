---
Title: CoMPS: Continual Meta Policy SearchManipulation
Date: 2021-09-08 10:20
Modified: Wednesday, 10. Oct 2021 02:06PM 
Category: Publication
Tags: Reinforcement Learning, Continual Learning 
Author: Glen Berseth
Published: International Conference on Learning Representations 2022
Authors: Glen Berseth, Zhiwei Zhang, Grace Zhang, Chelsea Finn, Sergey Levine
Cover: <div align="center">  <img width="100%" src="/projects/CoMPS/Continual Meta Policy Search.gif"> </div>
summary: We develop a new continual meta-learning method to address challenges in sequential multi-task learning. In this setting, the agent's goal is to achieve high reward over any sequence of tasks quickly. Prior meta-reinforcement learning algorithms have demonstrated promising results in accelerating the acquisition of new tasks. However, they require access to all tasks during training. Beyond simply transferring past experience to new tasks, our goal is to devise continual reinforcement learning algorithms that learn to learn, using their experience on previous tasks to learn new tasks more quickly. We introduce a new method, continual meta-policy search (CoMPS), that removes this limitation by meta-training in an incremental fashion, over each task in a sequence, without revisiting prior tasks. CoMPS continuously repeats two subroutines: learning a new task using RL and using the experience from RL to perform completely offline meta-learning to prepare for subsequent task learning. We find that CoMPS outperforms prior continual learning and off-policy meta-reinforcement methods on several sequences of challenging continuous control tasks. 
comments: true
Type: Continual Learning
TitleShort: Continual meta-reinforcement learning for lifelong learning
---

 <div align="center">  <img width="100%" src="/projects/CoMPS/Continual Meta Policy Search.gif"> </div>

We develop a new continual meta-learning method to address challenges in sequential multi-task learning. In this setting, the agent's goal is to achieve high reward over any sequence of tasks quickly. Prior meta-reinforcement learning algorithms have demonstrated promising results in accelerating the acquisition of new tasks. However, they require access to all tasks during training. Beyond simply transferring past experience to new tasks, our goal is to devise continual reinforcement learning algorithms that learn to learn, using their experience on previous tasks to learn new tasks more quickly. We introduce a new method, continual meta-policy search (CoMPS), that removes this limitation by meta-training in an incremental fashion, over each task in a sequence, without revisiting prior tasks. CoMPS continuously repeats two subroutines: learning a new task using RL and using the experience from RL to perform completely offline meta-learning to prepare for subsequent task learning. We find that CoMPS outperforms prior continual learning and off-policy meta-reinforcement methods on several sequences of challenging continuous control tasks. 


[Paper](https://arxiv.org/abs/2112.04467) </br>
[Project Website](https://sites.google.com/view/compspaper) </br>
