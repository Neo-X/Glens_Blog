---
Title: SMiRL: Surprise Minimizing RL in Dynamic Environments
Date: 2019-12-10 10:20
Modified: Wednesday, 10. Dec 2019 02:06PM 
Category: Publication
Tags: ReinforcementLearning, UnsupervisedLearning
Author: Glen Berseth
Authors: Glen Berseth, Daniel Geng, Coline Devin, Chelsea Finn, Dinesh Jayaraman, Sergey Levine 
Link: https://ccrig.github.io/
Cover: <div align="center"> <img width="300" src="/projects/CCRIG/method_step1.png" alt=""> <img alt="" width="300" src="/projects/CCRIG/cc_vae.png"> </div>
summary: All living organisms carve out environmental niches within which they can maintain relative predictability amidst the ever-increasing entropy around them [schneider1994, friston2009]. Humans, for example, go to great lengths to shield themselves from surprise --- we band together in millions to build cities with homes, supplying water, food, gas, and electricity to control the deterioration of our bodies and living spaces amidst heat and cold, wind and storm. The need to discover and maintain such surprise-free equilibria has driven great resourcefulness and skill in organisms across very diverse natural habitats. Motivated by this, we ask: could the motive of preserving order amidst chaos guide the automatic acquisition of useful behaviors in artificial agents? 
comments: true
---

<div align="center">
	<p>
				Glen Berseth, Daniel Geng, Coline Devin, Chelsea Finn, Dinesh Jayaraman, Sergey Levine
	</p>
	<p>	
            University of California, Berkeley, Stanford, Facebook
    </p>
</div>

<center>
All living organisms carve out environmental niches within which they can maintain relative predictability amidst the ever-increasing entropy around them [[schneider1994](http://www.ler.esalq.usp.br/aulas/lce1302/life_as_a_manifestation.pdf), [friston2009](https://www.fil.ion.ucl.ac.uk/~karl/The free-energy principle - a rough guide to the brain.pdf)]. Humans, for example, go to great lengths to shield themselves from surprise --- we band together in millions to build cities with homes, supplying water, food, gas, and electricity to control the deterioration of our bodies and living spaces amidst heat and cold, wind and storm. The need to discover and maintain such surprise-free equilibria has driven great resourcefulness and skill in organisms across very diverse natural habitats. Motivated by this, we ask: could the motive of preserving order amidst chaos guide the automatic acquisition of useful behaviors in artificial agents?



How might an agent in an environment acquire complex behaviors and skills with no external supervision? This central problem in artificial intelligence has evoked several candidate solutions, largely focusing on novelty-seeking behaviors [[schmidhuber1991](http://people.idsia.ch/~juergen/curioussingapore/curioussingapore.html), [bellemare2016](https://arxiv.org/abs/1606.01868), [Pathak2017](https://pathak22.github.io/noreward-rl/)]. In simulated worlds, such as video games, novelty-seeking intrinsic motivation can lead to interesting and meaningful behavior. However, we argue that these environments are fundamentally lacking compared to the real world. In the real world, natural forces and other agents offer bountiful novelty. The second law of thermodynamics stipulates ever-increasing entropy, and therefore perpetual novelty, without even requiring any agent intervention. Instead, the challenge in natural environments is allostasis: discovering behaviors that enable agents to maintain an equilibrium (homeostasis), for example to preserve their bodies, their homes, and avoid predators and hunger. 



We formalize homeostasis as an objective for reinforcement learning based on surprise minimization (SMiRL). In entropic and dynamic environments with undesirable forms of novelty, minimizing surprise (i.e., minimizing novelty) causes agents to naturally seek an equilibrium that can be stably maintained. Natural environments with winds, earthquakes, adversaries, and other disruptions already offer a steady stream of novel stimuli, and an agent that minimizes surprise in these environments will act and explore in order to find the means to maintain equilibrium in the face of these disturbances.



<div align="center">
            <img width="600" src="../projects/SMiRL/robotsurprise_stacked.png">
</div>



SMiRL is simple to describe and implement: it works by maintaining a density $p(\bs)$ of visited states and training a policy to act such that future states have high likelihood under $p(\bs)$. Across many different environments, with varied disruptive forces, and in agents with diverse embodiments and action spaces, we show that this simple approach induces useful equilibrium-seeking behaviors. We show that SMiRL agents can solve Tetris, avoid fireballs in Doom, and enable a simulated humanoid to balance and locomote, without any explicit task reward. More pragmatically, we show that SMiRL can be used together with a task reward to accelerate standard reinforcement learning in dynamic environments, and can provide a simple mechanism for imitation learning. SMiRL holds promise for a new kind of unsupervised RL method that produces behaviors that are closely tied to the prevailing disruptive forces, adversaries, and other sources of entropy in the environment.



<div align="center">
     <img width="600" src="../projects/SMiRL/SMiRL_Outline.png">
</div>

Here we show an illustration of the agent interaction loop using SMiRL. When the agent observes a state $\bs$, it computes the probability of this new state given the belief the agent has $r_{t} \leftarrow p_{\theta_{t-1}}(\bs)$. This belief can be thought of as a function that models the states the agent is most familiar with. Experiencing states that are more familiar will result in higher reward. After the agent experience a new state it updates its belief $p_{\theta_{t-1}}(\bs)$ over states to include the most recent experience. Then, the goal of the action policy $\pi(a|\bs, \theta_{t})$ is to choose actions that will result in agent consistently experiencing familiar states. Below, we visualize a policy trained to play the game of Tetris. On the left the blocks the agent chooses are shown and on the right is a visualization of $p_{\theta_{t}}(\bs)$. We can see how as the episode progresses the belief over possible locations to place blocks tends to favor only the bottom row. This encourages the agent to eliminate blocks to prevent board from filling up.



<div align="center">
     <img width="500" src="../projects/SMiRL/tetris/tetris_ps.gif">
</div>





**Emergent behaviour**



The SMiRL agent demonstrates meaningful emergent behaviors in each of these domains. In the Tetris environment, the agent is able to learn proactive behaviors to eliminate rows and properly play the game. The agent also learns emergent game playing behaviour in the *VizDoom* environment, acquiring an effective policy for dodging the fireballs thrown by the enemies. In both of these environments, stochastic and chaotic events force the SMiRL agent to take a coordinated course of action to avoid unusual states, such as full Tetris boards or fireball explorations.



Tetris

https://drive.google.com/file/d/1ZV_QWhSyksn0OIv6lCZgiq40D6LBe8xQ/view?usp=sharing



Doom hold line

Defend line

Haunted house:



<div align="center">
     <img width="25%" src="../projects/SMiRL/vizdoom/Doom_trained_enough_result.gif">
    <img width="25%" src="../projects/SMiRL/vizdoom/vizdoom_dtl.gif">
    <img width="30%" src="../projects/SMiRL/miniGrid/minigrid-maze-random-count.gif">
</div>





**Biped**



In the *Cliff* environment, the agent learns a policy that greatly reduces the probability of falling off of the cliff by bracing against the ground and stabilize itself at the edge, as shown in figure below. In the *Treadmill* environment, SMiRL learns a more complex locomotion behavior, jumping forward to increase the time it stays on the treadmill, as shown in figure below.



<div align="center">
    <img src="https://drive.google.com/file/d/1aAgD_FxHCpFcbI6OQD3q5Zv4OKfLvose/preview">
</div>



<div align="center">
	<iframe src="https://drive.google.com/file/d/1aAgD_FxHCpFcbI6OQD3q5Zv4OKfLvose/preview" width="320" height="240"></iframe>
</div>

Cliff: https://drive.google.com/file/d/1aAgD_FxHCpFcbI6OQD3q5Zv4OKfLvose/view?usp=sharing



Treadmill: https://drive.google.com/file/d/1Eo_runJaxMWU2-r2lARCdcrJAJEEFO9V/view?usp=sharing



Pedestal:

https://drive.google.com/file/d/1cK0NpBlBqYQDRtOSYNGgCYM4bTuocPjv/view





**Comparison to Intrinsic motivation:**



Below, we show plots of the environment-specific rewards over time on *Tetris*, *VizDoomTakeCover*, and the humanoid domains. In order to compare SMiRL to more standard intrinsic motivation methods, which seek out states that *maximize* surprise or novelty, we also evaluated ICM [[Pathak2017](https://pathak22.github.io/noreward-rl/)] and RND [[burda2018rnd](https://arxiv.org/abs/1810.12894)]. We include an oracle agent that directly optimizes the task reward. On *Tetris*, after training for $2000$ epochs, SMiRL achieves near perfect play, on par with the oracle reward optimizing agent, with no deaths. ICM seeks novelty by creating more and more distinct patterns of blocks rather than clearing them, leading to deteriorating game scores over time. On VizDoomTakeCover, SmiRL effectively learns to dodge fireballs thrown by the adversaries. 



![img](https://lh4.googleusercontent.com/AFN5hYDLH-mPGMya324L4kcCe_woIpGX9PzmF-h-wgbDO1wjbH5yRWTcb4WhGSEeY-DFozLOol-bpQ41BptTGP8U9fFdif5SrydC8vVVlPhs4FRoxi3acuKNbv_1mfGXCDiIio-_)





The baseline comparisons for the *Cliff* and *Treadmill* environments have a similar outcome. The novelty seeking behaviour of ICM causes it to learn a type of irregular behavior that causes the agent to jump off the *Cliff* and roll around on the *Treadmill*, maximizing the variety (and quantity) of falls.

![img](https://lh4.googleusercontent.com/COXfGWqy1R1hDfMYHpqEKxUsekX6EaSZtKjzDc2WjVMs7QqXzWiWCEyx4glM-umlD2iZqp48XHApXUn5jEJb_zQC-BblAMc4E4flgF1Shc325vL08wNhHcgBj2Yi5oCy3q3Yymfe)





While on the surface, SMiRL minimizes surprise and curiosity approaches like ICM maximize novelty, they are in fact not mutually incompatible. In particular, while ICM maximizes novelty with respect to a learned *transition* model, SMiRL minimizes surprise with respect to a learned *state* distribution. We can combine ICM and SMiRL to achieve even better results on the *Treadmill* environment. 





Biped walking:



https://drive.google.com/file/d/1vT-eGL0NzD2rF79h783OxiJaJHgHobJu/view

With ICM

https://drive.google.com/file/d/17FuCMHKVely_bVDV-QOYgz70O4_x8fG4/view



**Insights:**



The key insight utilized by our method is that, in contrast to simple simulated domains, realistic environments exhibit dynamic phenomena that gradually increase entropy over time. An agent that resists this growth in entropy must take active and coordinated actions, thus learning increasingly complex behaviors. This is different from commonly proposed intrinsic exploration methods based on novelty, which instead seek to visit novel states and increase entropy.



 Videos of our results are available at \url{https://sites.google.com/view/surpriseminimization}../files/bibtex/Dome.bib)